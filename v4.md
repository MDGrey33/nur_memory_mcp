# V4 Brief — Enhanced `hybrid_search` (Graph-backed Context Expansion, Minimal New Tools)

## Goal
Make `hybrid_search` behave like "portable memory" in chat by returning:
1) strong primary matches (as in V3)
2) an optional **related context pack** (graph expansion)
3) an **expand_options** hint so the assistant can say: "I can also expand with X/Y/Z if you like."

We will NOT add new user-facing tools except optional debug/health.

---

## High-level approach
Keep `hybrid_search` as the primary retrieval tool. Add an optional post-step that uses the graph to expand context around the top results.

### Current V3 (baseline)
- Embed query (OpenAI `text-embedding-3-large`)
- Vector search: `artifacts`, `artifact_chunks`, optional `memories` (Chroma)
- SQL FTS: `semantic_event.narrative` (Postgres)
- Merge with RRF, dedupe by artifact (prefer chunks)
- Optional neighbor expansion (±1 chunk)

### V4 additions
- Materialize a lightweight graph (inside Postgres) and use it for **graph_expand** in `hybrid_search`.
- `hybrid_search` returns **two sections**:
  - `primary_results`: the existing RRF winners
  - `related_context`: bounded 1-hop graph expansion results
- `hybrid_search` returns `expand_options` describing available knobs.

---

## Graph stack (minimal infra, permissive)
Use **Apache AGE inside existing Postgres** (no new container).
- Enable extension + create graph `nur`.
- Graph is derived from Postgres truth tables.

---

## Required data model changes (Postgres)

### New tables (entity resolution with quality-first approach)

#### 1) `entity` — Canonical entity registry with embedding support
```sql
CREATE TABLE entity (
    entity_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    entity_type TEXT NOT NULL,  -- person|org|project|object|place|other
    canonical_name TEXT NOT NULL,
    normalized_name TEXT NOT NULL,  -- lowercase, stripped for matching

    -- Rich context for deduplication (nullable, populated when available)
    role TEXT,           -- e.g., "Engineering Manager"
    organization TEXT,   -- e.g., "Acme Corp"
    email TEXT,          -- e.g., "alice@acme.com"

    -- Embedding for similarity-based dedup candidate search
    context_embedding vector(3072),

    -- Provenance
    first_seen_artifact_uid TEXT NOT NULL,
    first_seen_revision_id TEXT NOT NULL,

    -- For manual review queue (uncertain merges)
    needs_review BOOLEAN DEFAULT false,

    created_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

CREATE INDEX entity_type_name_idx ON entity(entity_type, normalized_name);
CREATE INDEX entity_embedding_idx ON entity
    USING ivfflat (context_embedding vector_cosine_ops) WITH (lists = 100);
CREATE INDEX entity_needs_review_idx ON entity(needs_review) WHERE needs_review = true;
```

#### 2) `entity_alias` — Known aliases for each entity
```sql
CREATE TABLE entity_alias (
    alias_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    entity_id UUID NOT NULL REFERENCES entity(entity_id) ON DELETE CASCADE,
    alias TEXT NOT NULL,
    normalized_alias TEXT NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT now(),

    UNIQUE(entity_id, normalized_alias)
);

CREATE INDEX entity_alias_lookup_idx ON entity_alias(normalized_alias);
```

#### 3) `entity_mention` — Every surface form occurrence (preserves evidence trail)
```sql
CREATE TABLE entity_mention (
    mention_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    entity_id UUID NOT NULL REFERENCES entity(entity_id) ON DELETE CASCADE,

    artifact_uid TEXT NOT NULL,
    revision_id TEXT NOT NULL,
    surface_form TEXT NOT NULL,  -- Exact text as it appeared in document

    -- Character offsets for evidence linking (nullable if not available)
    start_char INT,
    end_char INT,

    created_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

CREATE INDEX entity_mention_entity_idx ON entity_mention(entity_id);
CREATE INDEX entity_mention_revision_idx ON entity_mention(artifact_uid, revision_id);
```

#### 4) `event_actor` — Structured actor relationships
```sql
CREATE TABLE event_actor (
    event_id UUID NOT NULL REFERENCES semantic_event(event_id) ON DELETE CASCADE,
    entity_id UUID NOT NULL REFERENCES entity(entity_id) ON DELETE CASCADE,
    role TEXT NOT NULL,  -- owner|contributor|reviewer|stakeholder|other

    PRIMARY KEY (event_id, entity_id)
);

CREATE INDEX event_actor_entity_idx ON event_actor(entity_id);
```

#### 5) `event_subject` — Structured subject relationships
```sql
CREATE TABLE event_subject (
    event_id UUID NOT NULL REFERENCES semantic_event(event_id) ON DELETE CASCADE,
    entity_id UUID NOT NULL REFERENCES entity(entity_id) ON DELETE CASCADE,

    PRIMARY KEY (event_id, entity_id)
);

CREATE INDEX event_subject_entity_idx ON event_subject(entity_id);
```

> Keep V3 `semantic_event.actors_json` and `subject_json` unchanged for backwards compatibility. These new tables are the structured layer for graph.

---

## Entity Resolution Strategy (Quality-First)

### Extraction: Rich entity context in prompt
Extend the extraction prompt to return entities with context clues:

```json
{
  "events": [...],
  "entities_mentioned": [
    {
      "surface_form": "Alice Chen",
      "canonical_suggestion": "Alice Chen",
      "type": "person",
      "context_clues": {
        "role": "Engineering Manager",
        "org": "Acme Corp",
        "email": "achen@acme.com"
      },
      "aliases_in_doc": ["Alice", "A. Chen"],
      "confidence": 0.95
    }
  ]
}
```

The LLM has document context to make good canonicalization decisions. Extracting role/org/email provides strong deduplication signals.

### Deduplication: Two-Phase (Embedding + LLM Confirmation)

**Phase A: Candidate Generation (fast, embedding-based)**
1. Generate embedding from: `"{canonical_name}, {type}, {role}, {org}"`
2. Query existing entities of same type with cosine similarity > 0.85
3. If candidates found, proceed to Phase B
4. If no candidates, create new entity

**Phase B: LLM Confirmation (high-stakes merge decision)**
For each candidate pair, call LLM with both contexts:

```
You are determining if two entity mentions refer to the same real-world entity.

Entity A (from document "{title_a}"):
- Name: "Alice Chen"
- Type: person
- Context: Engineering Manager at Acme Corp, email achen@acme.com

Entity B (from document "{title_b}"):
- Name: "A. Chen"
- Type: person
- Context: Manager, mentioned alongside Acme project

Are these the same entity? Return JSON:
{"decision": "same|different|uncertain", "canonical_name": "...", "reason": "..."}
```

**Decision Rules:**
- `same` → Merge: add alias to existing entity, link mention to existing entity_id
- `different` → Create new entity
- `uncertain` → Create new entity, add `(:Entity)-[:POSSIBLY_SAME]->(:Entity)` edge for later resolution

**Why this approach:**
- Embedding pre-filter is fast and reduces LLM calls to O(candidates), not O(n²)
- LLM confirmation with context prevents false merges ("Alice Chen" vs "Bob Chen")
- "Uncertain" pathway avoids premature decisions while preserving signal
- Cost is ~$0.001 per candidate pair (gpt-4o-mini)

---

## Graph Model (Simplified — No Revision Nodes)

After analysis, Revision nodes add complexity without proportional value. Use a simpler model:

### Nodes
```cypher
(:Entity {entity_id, canonical_name, type, role, organization})
(:Event {event_id, category, narrative, artifact_uid, revision_id, event_time, confidence})
```

### Edges
```cypher
(:Entity)-[:ACTED_IN {role}]->(:Event)
(:Event)-[:ABOUT]->(:Entity)
(:Entity)-[:POSSIBLY_SAME {confidence, reason}]->(:Entity)  -- For uncertain merges
```

### Why no Revision nodes?
- "Events in same revision" is trivially queryable: `MATCH (e:Event {artifact_uid: $uid, revision_id: $rev})`
- Reduces graph complexity and MERGE operations
- All revision info available via Event properties

### Idempotency
- Upsert with Cypher `MERGE` on stable ids
- Re-run is safe

---

## Processing Pipeline (Revised)

### Flow (entity resolution during extraction, not separate)

```
artifact_ingest
    │
    ▼
event_jobs.enqueue(job_type='extract_events')
    │
    ▼
Worker: extract_events (EXTENDED)
    ├── Prompt A: Extract events + entities_mentioned per chunk
    ├── Prompt B: Canonicalize events across chunks
    ├── For each entity_mentioned:
    │     ├── Generate context embedding
    │     ├── Find candidates (embedding similarity > 0.85, same type)
    │     ├── If candidates: LLM confirmation call
    │     ├── Decision: merge/create/uncertain
    │     └── Insert entity + entity_alias + entity_mention
    ├── Write semantic_event + event_evidence (existing)
    ├── Write event_actor + event_subject (linking to resolved entity_ids)
    └── Enqueue graph_upsert (SAME TRANSACTION — avoids race condition)
    │
    ▼
Worker: graph_upsert
    ├── MERGE Entity nodes from entity table
    ├── MERGE Event nodes from semantic_event
    ├── MERGE ACTED_IN edges from event_actor
    ├── MERGE ABOUT edges from event_subject
    └── MERGE POSSIBLY_SAME edges for uncertain matches
```

### Why entity resolution in extract_events?
- Keeps pipeline atomic — no race conditions
- Entity context is freshest during extraction
- Single job handles both extraction and resolution
- graph_upsert becomes a pure materialization step

### Job types in `event_jobs`
- `job_type = 'extract_events'` (existing, extended)
- `job_type = 'graph_upsert'` (new)

### Notes
- Graph is a **materialized index**, not source of truth
- Re-run is safe (MERGE semantics)
- Entity tables are the source of truth for entities

---

## Enhanced `hybrid_search` API (core deliverable)

### New parameters
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `graph_expand` | bool | false | Enable graph-based context expansion |
| `graph_depth` | int | 1 | Expansion depth (only 1 supported in V4) |
| `graph_budget` | int | 10 | Max additional related items |
| `graph_seed_limit` | int | 1 | How many primary results to use as expansion seeds |
| `graph_filters` | list[str] | ["Decision","Commitment","QualityRisk"] | Category filter (ignored unless graph_expand=true) |
| `include_entities` | bool | true | Include entity list when graph_expand=true |
| `include_revision_diff` | bool | false | Optional (feature-flag): include revision diffs in graph expansion payloads |

### Output shape (must be stable)
Return JSON with:

#### 1) `primary_results[]`
Existing results format (chunks/artifacts/events) with evidence pointers. **Unchanged from V3.**

#### 2) `related_context[]`
Additional graph-derived items, each with:
```json
{
  "type": "event",
  "id": "uuid",
  "category": "Decision",
  "reason": "same_actor:Alice Chen",
  "summary": "Team decided to use Postgres for the backend",
  "event_time": "2024-03-15T14:30:00Z",
  "evidence": [
    {"quote": "we decided on Postgres", "artifact_uid": "...", "start_char": 450, "end_char": 480}
  ]
}
```

Standardized `reason` values:
- `same_actor:{name}` — Connected via shared actor
- `same_subject:{name}` — Connected via shared subject
- `1_hop_via:{category}` — Connected through graph traversal

#### 3) `entities[]` (if include_entities=true)
Canonical entities involved in the response:
```json
{
  "entity_id": "uuid",
  "name": "Alice Chen",
  "type": "person",
  "role": "Engineering Manager",
  "organization": "Acme Corp",
  "aliases": ["Alice", "A. Chen"],
  "mention_count": 5
}
```

#### 4) `expand_options[]`
This is a UX hint: `hybrid_search` must return an `expand_options[]` field so the assistant can say:
“I can expand hybrid search with X/Y/Z if you like.”

##### Response field
- `expand_options: ExpandOption[]` (top-level, returned on every call)

##### ExpandOption schema
Each item:
- `name` (string, required): stable key used as request parameter
- `type` (string, required): boolean | integer | string[] | string
- `default` (any, required): default value if caller omits
- `description` (string, required): user-facing explanation
- `effect` (string, optional): what changes in output/behavior
- `constraints` (string, optional): dependencies/validation notes

##### Required options (must always be present)
- `include_memory` (default false)
- `expand_neighbors` (default false)
- `include_events` (default true)
- `graph_expand` (default true)
- `graph_filters` (default ["Decision","Commitment","QualityRisk"])
- `graph_budget` (default 10, range 0–50)
- `include_entities` (default true when graph_expand)
- `include_revision_diff` (default false, optional feature flag)

##### Validation rules
- `graph_filters` values must be from:
  Commitment, Execution, Decision, Collaboration, QualityRisk, Feedback, Change, Stakeholder
- If `graph_expand=false`:
  ignore `graph_filters`, `graph_budget`, `include_entities`, `include_revision_diff`
- `expand_neighbors` affects only chunk hits; otherwise no-op

##### Behavior requirement
`expand_options` is static capability metadata and must not depend on results.
It exists to guide UX and follow-up prompting.

### Ranking rules
- **primary_results**: unchanged (RRF)
- **related_context**: rank by:
  1. Event recency (if timestamp exists)
  2. Confidence score
  3. Category priority (Decision > Commitment > QualityRisk > others)

---

## Graph Expansion Algorithm

```python
async def expand_via_graph(primary_results, params):
    seed_events = []

    # Step 1: Collect seed event_ids from top primary results
    # NOTE: When include_events=true, prefer seeding from Postgres semantic_event hits
    # (higher precision) and fall back to mapping vector hits (artifact/chunk)
    # -> artifact_revision -> events.
    for result in primary_results[:params.graph_seed_limit]:
        if result.type == "event":
            seed_events.append(result.event_id)
        elif result.type in ("chunk", "artifact"):
            # Map chunk/artifact to events via artifact_revision table
            # JOIN on artifact_revision.artifact_id = result.artifact_id
            revision = await get_revision_for_artifact(result.artifact_id)
            events = await get_events_for_revision(
                revision.artifact_uid,
                revision.revision_id
            )
            seed_events.extend([e.event_id for e in events])

    # Step 2: Cypher 1-hop expansion
    cypher = """
    MATCH (seed:Event) WHERE seed.event_id IN $seed_ids

    // Get entities connected to seed events
    OPTIONAL MATCH (seed)<-[:ACTED_IN]-(actor:Entity)
    OPTIONAL MATCH (seed)-[:ABOUT]->(subject:Entity)

    WITH seed, collect(DISTINCT actor) + collect(DISTINCT subject) AS entities
    UNWIND entities AS entity

    // Get other events connected to those entities (1 hop)
    MATCH (entity)-[:ACTED_IN|ABOUT]-(related:Event)
    WHERE NOT related.event_id IN $seed_ids
      AND ($category_filter IS NULL OR related.category IN $category_filter)

    RETURN DISTINCT related, entity,
           CASE
             WHEN (entity)-[:ACTED_IN]->(related) THEN 'same_actor:' + entity.canonical_name
             ELSE 'same_subject:' + entity.canonical_name
           END AS reason
    ORDER BY related.event_time DESC NULLS LAST, related.confidence DESC
    LIMIT $budget
    """

    related = await age_query(cypher, {
        "seed_ids": seed_events,
        "category_filter": params.graph_filters,
        "budget": params.graph_budget
    })

    # Step 3: Fetch full event data + evidence from Postgres
    related_context = []
    for row in related:
        event = await fetch_event_with_evidence(row.related.event_id)
        related_context.append({
            "type": "event",
            "id": str(event.event_id),
            "category": event.category,
            "reason": row.reason,
            "summary": event.narrative,
            "event_time": event.event_time.isoformat() if event.event_time else None,
            "evidence": [format_evidence(e) for e in event.evidence]
        })

    # Step 4: Collect unique entities across all results
    all_event_ids = seed_events + [r["id"] for r in related_context]
    entities = await fetch_entities_for_events(all_event_ids)

    return {
        "related_context": related_context,
        "entities": entities
    }
```

### Chunk-to-Revision Mapping
Chunks in ChromaDB have `artifact_id` but not `revision_id`. Use:
```sql
SELECT artifact_uid, revision_id
FROM artifact_revision
WHERE artifact_id = $artifact_id AND is_latest = true
```

---

## Optional debug endpoints/tools (non-user-facing)

- `graph_health` — extension enabled, node/edge counts, POSSIBLY_SAME count
- `job_status` — already exists; ensure it includes graph_upsert status
- `entity_review_queue` — list entities with needs_review=true (for manual disambiguation)

---

## E2E Tests

### Test 1: Entity extraction returns rich context
- Ingest artifact mentioning "Alice Chen, Engineering Manager at Acme"
- Assert entity created with role="Engineering Manager", organization="Acme Corp"
- Assert entity_mention links to correct character offsets

### Test 2: Entity deduplication (same person)
- Ingest doc A mentioning "Alice Chen, Engineering Manager"
- Ingest doc B mentioning "A. Chen from Acme"
- Assert: single entity created (merged)
- Assert: both surface forms in entity_mention
- Assert: alias "A. Chen" added to entity_alias

### Test 3: Entity deduplication (different people)
- Ingest doc mentioning "Alice Chen" (Engineer at Acme)
- Ingest doc mentioning "Alice Chen" (Designer at OtherCorp)
- Assert: two separate entities created (different org context)

### Test 4: Uncertain merge creates POSSIBLY_SAME edge
- Ingest doc mentioning "A. Chen" with minimal context
- Ingest doc mentioning "Alice C." with minimal context
- Assert: two entities created
- Assert: POSSIBLY_SAME edge exists in graph
- Assert: at least one entity has needs_review=true

### Test 5: graph_upsert materializes nodes/edges
- Ingest artifact → wait extraction DONE → wait graph_upsert DONE
- Assert: Entity and Event nodes exist in AGE graph
- Assert: ACTED_IN and ABOUT edges exist
- Assert: hybrid_search(graph_expand=true) returns non-empty related_context

### Test 6: hybrid_search returns expand_options
- Call hybrid_search
- Assert: expand_options contains graph_expand, include_memory, expand_neighbors, graph_budget, graph_filters

### Test 7: Related context is connected and bounded
- Create two docs sharing the same actor (verified same entity)
- Call hybrid_search(query about doc A, graph_expand=true)
- Assert: related_context includes events from doc B
- Assert: related_context.length <= graph_budget
- Assert: each related item has standardized reason format

### Test 8: graph_seed_limit respected
- Create 10 primary results
- Call hybrid_search(graph_expand=true, graph_seed_limit=3)
- Assert: expansion only uses top 3 results as seeds

### Test 9: Backward compatibility
- Call hybrid_search(graph_expand=false)
- Assert: output shape identical to V3 (no related_context, no entities)
- Assert: primary_results quality unchanged

### Test 10: Chunk-to-revision mapping
- Ingest large artifact (creates chunks)
- Search returns a chunk as primary result
- Call with graph_expand=true
- Assert: correctly maps chunk → artifact_revision → events → graph expansion

---

## Definition of Done

### Entity Resolution
- [ ] Extraction prompt returns `entities_mentioned` with context clues (role, org, email)
- [ ] Entity table includes `context_embedding` vector column
- [ ] Entity deduplication uses embedding similarity (>0.85) + LLM confirmation
- [ ] `entity_mention` table tracks every surface form with character offsets
- [ ] `entity_alias` table stores known aliases per entity
- [ ] `POSSIBLY_SAME` edges created for uncertain merges
- [ ] `needs_review` flag set on entities requiring manual disambiguation

### Graph
- [ ] AGE extension enabled in Postgres
- [ ] Graph `nur` created with Entity and Event nodes
- [ ] ACTED_IN, ABOUT, POSSIBLY_SAME edges materialized
- [ ] graph_upsert job type works with idempotent MERGE
- [ ] No Revision nodes (simplified model)

### hybrid_search
- [ ] Supports all new parameters: graph_expand, graph_depth, graph_budget, graph_seed_limit, graph_filters, include_entities
- [ ] `graph_filters` defaults to null (all categories)
- [ ] Returns primary_results (unchanged from V3)
- [ ] Returns related_context with standardized reason format
- [ ] Returns entities list with aliases and mention counts
- [ ] Returns expand_options for progressive disclosure
- [ ] Chunk-to-revision mapping works via artifact_revision.artifact_id join

### Quality & Performance
- [ ] E2E tests pass (all 10 tests)
- [ ] Same person across documents correctly deduplicated
- [ ] Different people with similar names stay separate
- [ ] Backward compatibility: graph_expand=false returns V3 shape
- [ ] Latency: graph_expand adds < 300ms for typical queries

### Cost (per document, quality path)
- Extraction: ~$0.015 (gpt-4o-mini, includes entity extraction)
- Entity embeddings: ~$0.0001 per entity
- Entity dedup LLM calls: ~$0.001 per candidate pair
- **Total: ~$0.02/document** — acceptable for quality-first approach
