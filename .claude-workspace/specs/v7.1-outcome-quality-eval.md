# V7.1 Specification: Outcome Quality Evaluation (Value > Mechanics)

**Version**: 7.1.0  
**Status**: Draft  
**Created**: 2026-01-01  
**Author**: Claude Development Team  

---

## Executive Summary

V7.1 defines **outcome-based quality evaluation** for MCP Memory: instead of judging whether internal event/entity/graph representations match a ground-truth schema, we judge whether the system produces **correct, decision-useful answers with evidence** and whether **events + graph expansion measurably improve outcomes vs a simple RAG baseline**.

This replaces “useless” tests (those dominated by ID/format/paraphrase mismatch) with a benchmark that directly measures user value for **personal life + business decisions**.

---

## Problem Statement

### What’s wrong with the current approach (V7 MVP)

The V7 benchmark suite proved it can run deterministically (replay fixtures), but the **event/entity/graph quality scores** can be misleading because they are sensitive to:

- **Representation mismatch** (IDs vs names, different schemas)
- **Paraphrases** (LLM narratives not equal to ground truth strings)
- **Different “connection” semantics** (what the system expands vs what ground truth expects)

These failures can incorrectly imply “system is bad” even when user-visible answers might improve—or conversely, can pass a metric while still producing poor answers.

### What we actually need to know

1. **Is the system better than a simple RAG?**
2. **Do events + graph expansion improve answer correctness/completeness without increasing hallucinations?**
3. **Are outputs supported by evidence from source documents?**

---

## Definitions

### System Under Test (SUT)

The full MCP Memory retrieval system (V6/V6.1+), including:

- `remember()` ingestion (event extraction pipeline)
- `recall()` retrieval, optional event inclusion, optional expansion
- Postgres-backed “graph expansion” (SQL joins from events/entities)

### Baseline (“Simple RAG”)

The baseline run is **content-only retrieval** with expansion and events disabled:

- `recall(expand=False, include_events=False, include_entities=False)`  
- Same query, same `limit`, same index/storage

### Full System Mode

The full mode run uses the intended advanced features:

- `recall(expand=True, include_events=True, include_entities=True)`  
- Same query and limit as baseline

> Note: if `include_entities` is not supported/meaningful in your current response format, omit it. The evaluation still works as long as citations/evidence are present.

---

## What existing V7 tests are **not** gates (keep as diagnostics)

These remain useful for debugging and drift detection, but are **not** the primary “value gate”:

- Event extraction F1 / entity F1 / graph connection F1 against schema-based ground truth
- ID mapping correctness tests (mechanical)
- Metric math unit tests (`benchmarks/tests/test_metrics.py`) (keep: deterministic correctness)

---

## V7.1 Outcome Evaluation (Primary)

### Key idea

Measure success by **answer quality with evidence**, and measure value by **A/B comparison**:

- **A**: baseline simple RAG output
- **B**: full system output

Score which one is more correct/complete and whether either hallucinates.

### Evaluation artifacts

Add a new dataset:

```
.claude-workspace/benchmarks/value_eval/
├── cases.json              # Outcome test cases (questions + required facts + evidence rules)
├── judge_rubric.md         # Strict scoring rubric and definitions
└── reports/                # Generated results
```

### Case schema (`cases.json`)

Each case defines what must be true in a good answer and how it must be supported.

```json
{
  "version": "1.0",
  "cases": [
    {
      "id": "ve_001",
      "question": "What commitments does Bob have and when are they due?",
      "required_facts": [
        {
          "fact_id": "bob_api_refactor_due",
          "claim": "Bob committed to complete the API refactor by March 25th, 2024",
          "must_cite": [
            {
              "doc": "meetings/meeting_001.txt",
              "quote_contains": "Bob: Complete API refactor by March 25th"
            }
          ]
        }
      ],
      "forbidden_claims": [
        "Bob committed to something not present in the corpus"
      ]
    }
  ]
}
```

**Design rules**

- Required facts must be **atomic** (one actor + one action + one date/value).
- Each required fact must include **evidence requirements** (doc + quote substring).
- Include some **negative cases** (questions where the correct answer is “unknown/not stated”).

---

## Judge (What it is and how it works)

### Judge definition

The judge is a separate evaluation step (typically a strong LLM) that grades:

- Baseline answer (A)
- Full-system answer (B)

against the test case’s required facts + evidence requirements.

### Judge inputs

For each case:

- `question`
- `required_facts` (+ evidence rules)
- `baseline_answer` (A) + citations/evidence
- `full_answer` (B) + citations/evidence

Optionally include the source docs (or doc excerpts) if you want to detect hallucinations more strictly.

### Judge outputs (structured JSON)

```json
{
  "case_id": "ve_001",
  "baseline": {
    "correctness": 1,
    "completeness": 1,
    "evidence": 0,
    "hallucination": 1,
    "missing_facts": ["bob_api_refactor_due"],
    "unsupported_claims": ["..."]
  },
  "full": {
    "correctness": 2,
    "completeness": 2,
    "evidence": 2,
    "hallucination": 2,
    "missing_facts": [],
    "unsupported_claims": []
  },
  "delta": {
    "full_beats_baseline": true,
    "notes": "Full system cited the correct quote; baseline did not."
  }
}
```

---

## Scoring Rubric (strict)

Scores are integer (0–2). A case “passes” only if the **Full** answer meets minimum quality.

- **Correctness (0–2)**:
  - 2: all required facts present and correct
  - 1: some correct, some wrong/unclear
  - 0: materially wrong
- **Completeness (0–2)**:
  - 2: all required facts included
  - 1: some missing
  - 0: mostly missing
- **Evidence support (0–2)**:
  - 2: each required fact has correct evidence per `must_cite`
  - 1: some evidence, incomplete
  - 0: no valid evidence
- **Hallucinations (0–2)**:
  - 2: no unsupported claims
  - 1: minor unsupported claims
  - 0: major hallucinations impacting decisions

### Primary KPI

**Value Win Rate**: % of cases where **Full beats Baseline** on correctness/completeness **without increasing hallucinations**.

---

## Execution Plan (do not implement in this doc)

### PR CI (fast + deterministic)

- Run the existing V7 replay suite for mechanical correctness.
- Run V7.1 outcome eval in **replay mode** using frozen answers (or frozen retrieval outputs) **only as regression detection**, not absolute quality gating.

### Nightly / Release (live + real quality)

- Run V7.1 outcome eval in **live mode**
- Track trends over time; gate releases if value win rate drops or hallucinations rise.

---

## Decision Points

1. **Judge model**: which LLM(s) evaluate answers?
2. **Evidence format**: quotes vs spans vs doc IDs (quotes recommended).
3. **Baseline definition**: confirm exact `recall()` flags/params for “simple RAG”.
4. **Pass thresholds**: define acceptable hallucination rate and required value win rate.

---

## Success Criteria (V7.1)

- [ ] `cases.json` contains an MVP set (20–30 cases) spanning decisions, commitments, risks, and negative cases
- [ ] Each required fact has evidence requirements
- [ ] Outcome eval produces structured judge reports and aggregates
- [ ] We can answer the question: **Full system vs baseline—who wins?**


