# Nur Memory MCP — V3 Full Context (Semantic Events)  
**Scope:** Add *semantic events* on top of V2 artifacts/memory, with **minimal infra** and **no Kafka**.  
**Deployment target:** single machine, Docker Compose, low load.

---

## 1) Current State (V2 Baseline)

### 1.1 What exists today
- **MCP Server**: Python (FastMCP) + Starlette/Uvicorn, Streamable HTTP (SSE + JSON-RPC)
- **Vector DB**: **ChromaDB HTTP server** (documented as `localhost:8100`)
- **Embeddings**: OpenAI `text-embedding-3-large` (best quality)
- **Collections**:
  - `memory` (curated memories)
  - `history` (conversation turns)
  - `artifacts` (full text for small artifacts)
  - `artifact_chunks` (chunked text for large artifacts)
- **Chunking policy**:
  - ≤ `SINGLE_PIECE_MAX_TOKENS` → store as one piece (no chunking)
  - > threshold → token-window chunking (`CHUNK_TARGET_TOKENS`, `CHUNK_OVERLAP_TOKENS`)
- **Search**:
  - `artifact_search` / `hybrid_search` already exist for evidence retrieval (RRF merge, neighbor expansion optional)

### 1.2 What V2 does NOT do
- No structured “events” (decisions, deliveries, feedback, etc.) extracted into a DB
- No version history for documents as immutable revisions (unless you’ve already added it informally)
- No background processing pipeline (beyond ingest + embeddings)

---

## 2) V3 Goal (What we are building)

### 2.1 Outcome
A pipeline that turns artifacts (emails/docs/chats/transcripts) into **semantic events**:

- **Artifacts** = immutable evidence (full fidelity)
- **Events** = structured, queryable “what happened” items, each with:
  - category (Decision, Execution, Feedback…)
  - who/what it’s about (subject)
  - who participated (actors + roles)
  - time (if present)
  - narrative summary
  - **evidence pointers** (artifact UID + revision + chunk + offsets + short quote)
  - confidence

### 2.2 Minimal infra decision
- **Drop Kafka**.
- Use **Postgres as:**
  1) **Event DB** (source of truth for events)
  2) **Durable job queue** (Kafka-esque) to trigger event extraction asynchronously

This keeps ingestion fast and makes event extraction reliable + retryable.

---

## 3) V3 New Capabilities (Feature List)

### 3.1 Event extraction (core)
- After artifact ingestion, V3 creates a **job** to extract semantic events.
- A worker processes jobs asynchronously:
  - loads artifact text/chunks from Chroma
  - calls LLM with strict JSON prompts
  - writes canonical events + evidence spans into Postgres in one transaction

### 3.2 Evidence-backed event retrieval (UX-visible)
- New MCP tools:
  - `event_search` (query events)
  - `event_get` (fetch an event + evidence)
  - `event_list_for_revision` (events for one artifact revision)
  - `event_reextract` (force re-extraction)
  - `job_status` (to show “events pending/done/failed”)

### 3.3 Artifact revisioning (strongly recommended in V3)
- Keep multiple revisions of the “same” document:
  - `artifact_uid` (stable)
  - `revision_id` (immutable snapshot)
- Re-ingest of unchanged revision: **no-op**
- New revision: enqueue extraction

### 3.4 Operational reliability
- Idempotency guaranteed:
  - unique job key per (artifact_uid, revision_id)
  - replace events for that revision on successful re-run (deterministic final state)
- Retry policy for transient failures (OpenAI / network)
- No partial writes: events are written only after extraction succeeds.

---

## 4) V3 Architecture

### 4.1 Services (Docker Compose on one host)
- `mcp-server`
- `chroma`
- `postgres` (event DB + job queue)
- `event-worker` (same codebase image is fine)

### 4.2 Data flow
1) **MCP tool** `artifact_ingest` writes:
   - artifact record to `artifacts` (if small)
   - or chunk records to `artifact_chunks` (if large)
2) `artifact_ingest` upserts `artifact_revision` in Postgres
3) `artifact_ingest` enqueues `event_job` in Postgres
4) `event-worker` claims jobs via `SELECT … FOR UPDATE SKIP LOCKED`
5) worker:
   - fetches text from Chroma
   - runs Prompt A (extract) chunk-wise
   - runs Prompt B (canonicalize) for the revision
   - writes `semantic_event` + `event_evidence` in one DB transaction
6) MCP tools serve event queries from Postgres (optionally hybrid with Chroma later)

### 4.3 Diagram (conceptual)

Claude/Client
|
|  MCP (SSE/JSON-RPC)
v
mcp-server  —> OpenAI (embeddings + extraction model)
|  
|   --> ChromaDB (artifacts, chunks, memory, history)
|
+–> Postgres (artifact_revision + event_jobs + semantic_event + event_evidence)
^
|
event-worker (polls/claims jobs, writes events)

---

## 5) Data Model (Postgres)

### 5.1 artifact revisions
**Table: `artifact_revision`**
- `artifact_uid` TEXT NOT NULL
- `revision_id` TEXT NOT NULL
- `artifact_id` TEXT NOT NULL              # the Chroma artifact id (e.g., art_9f2c)
- `artifact_type` TEXT NOT NULL            # email|doc|chat|transcript|note
- `source_system` TEXT NOT NULL
- `source_id` TEXT NOT NULL
- `source_ts` TIMESTAMPTZ NULL
- `ingested_at` TIMESTAMPTZ NOT NULL DEFAULT now()
- `content_hash` TEXT NOT NULL
- `token_count` INT NOT NULL
- `is_chunked` BOOLEAN NOT NULL
- `chunk_count` INT NOT NULL
- `sensitivity` TEXT NOT NULL              # normal|sensitive|highly_sensitive
- `visibility_scope` TEXT NOT NULL         # me|team|org|custom
- `retention_policy` TEXT NOT NULL         # forever|1y|until_resolved|custom
- `is_latest` BOOLEAN NOT NULL DEFAULT true

**PK:** (`artifact_uid`, `revision_id`)  
**Index:** (`artifact_uid`, `is_latest`), (`ingested_at`)

> Best practice: when a new revision is inserted, set previous revisions for that `artifact_uid` to `is_latest=false`.

---

### 5.2 durable job queue
**Table: `event_jobs`**
- `job_id` UUID PK
- `job_type` TEXT NOT NULL DEFAULT 'extract_events'
- `artifact_uid` TEXT NOT NULL
- `revision_id` TEXT NOT NULL
- `status` TEXT NOT NULL                  # PENDING|PROCESSING|DONE|FAILED
- `attempts` INT NOT NULL DEFAULT 0
- `max_attempts` INT NOT NULL DEFAULT 5
- `next_run_at` TIMESTAMPTZ NOT NULL DEFAULT now()
- `locked_at` TIMESTAMPTZ NULL
- `locked_by` TEXT NULL
- `last_error_code` TEXT NULL
- `last_error_message` TEXT NULL
- `created_at` TIMESTAMPTZ NOT NULL DEFAULT now()
- `updated_at` TIMESTAMPTZ NOT NULL DEFAULT now()

**Unique:** (`artifact_uid`, `revision_id`, `job_type`)  
**Index:** (`status`, `next_run_at`)

**Claiming jobs:** `SELECT … FOR UPDATE SKIP LOCKED` (safe multi-worker concurrency)

---

### 5.3 events
**Table: `semantic_event`**
- `event_id` UUID PK
- `artifact_uid` TEXT NOT NULL
- `revision_id` TEXT NOT NULL
- `category` TEXT NOT NULL                # fixed taxonomy
- `event_time` TIMESTAMPTZ NULL
- `narrative` TEXT NOT NULL               # 1–2 sentences
- `subject_json` JSONB NOT NULL
- `actors_json` JSONB NOT NULL
- `confidence` DOUBLE PRECISION NOT NULL
- `extraction_run_id` UUID NOT NULL
- `created_at` TIMESTAMPTZ NOT NULL DEFAULT now()

**Index:** (`artifact_uid`, `revision_id`), (`category`, `event_time`)  
Optional: Postgres FTS index on `narrative`

---

### 5.4 evidence spans
**Table: `event_evidence`**
- `event_id` UUID FK → semantic_event(event_id) ON DELETE CASCADE
- `artifact_uid` TEXT NOT NULL
- `revision_id` TEXT NOT NULL
- `chunk_id` TEXT NULL
- `start_char` INT NOT NULL
- `end_char` INT NOT NULL
- `quote` TEXT NOT NULL                  # <= 25 words
- `created_at` TIMESTAMPTZ NOT NULL DEFAULT now()

**Index:** (`event_id`), (`artifact_uid`, `revision_id`)

---

## 6) Event Taxonomy (Fixed Categories)

Use exactly:
- `Commitment`
- `Execution`
- `Decision`
- `Collaboration`
- `QualityRisk`
- `Feedback`
- `Change`
- `Stakeholder`

This keeps cross-domain compatibility (work + personal life).

---

## 7) MCP Tooling (V3 Additions)

### 7.1 `event_search`
**Purpose:** query structured events (not raw text).

Inputs:
- `query` (optional)
- `limit` (default 20)
- filters: `category?`, `time_from?`, `time_to?`, `artifact_uid?`
- `include_evidence` (default true)

Implementation (V3.0):
- Start with Postgres FTS on `narrative` + filters.
- (Optional V3.1) add semantic search via Chroma `event_summaries` if you want “vector search on events”.

Outputs:
- list of events with narrative + key fields + evidence pointers

### 7.2 `event_get`
Fetch full event record + evidence list.

### 7.3 `event_list_for_revision`
Return events for (`artifact_uid`, `revision_id`), defaulting to latest revision if not provided.

### 7.4 `event_reextract`
Force re-extraction by enqueuing a job even if one exists, or by setting a `force=true` flag and replacing events for that revision.

### 7.5 `job_status`
Return job status for an artifact revision:
- PENDING (not processed yet)
- PROCESSING
- DONE
- FAILED (with error + next retry time if any)

> UX note: This avoids confusion because events are asynchronous.

---

## 8) Worker Behavior (Exact)

### 8.1 Enqueue (inside `artifact_ingest`)
After Chroma writes succeed:
1) Upsert `artifact_revision`
2) Insert job into `event_jobs`
   - ON CONFLICT DO NOTHING unless `force=true`

### 8.2 Claim loop
Repeat every `POLL_INTERVAL_MS`:
1) BEGIN
2) SELECT pending jobs where `next_run_at <= now()`
   - ORDER BY priority (optional), created_at
   - `FOR UPDATE SKIP LOCKED`
3) UPDATE selected rows: `status=PROCESSING`, `locked_at=now()`, `locked_by=WORKER_ID`, `attempts=attempts+1`
4) COMMIT

### 8.3 Execute job
Given (artifact_uid, revision_id):
1) Load `artifact_revision` record
2) Fetch text from Chroma:
   - if `is_chunked=false`: fetch artifact content
   - if `is_chunked=true`: fetch all chunks ordered by `chunk_index`
3) For each chunk:
   - call Prompt A (extract) → JSON
   - validate strictly (schema)
4) Call Prompt B (canonicalize) for the revision
5) Atomic DB write:
   - BEGIN
   - DELETE existing events for (`artifact_uid`,`revision_id`)  **(recommended: replace always)**
   - INSERT canonical events + evidence spans
   - COMMIT
6) Mark job DONE

### 8.4 Failure + retry
- If OpenAI fails: no DB writes; update job FAILED or reschedule
- Retry policy (recommended):
  - transient errors: set `next_run_at` with backoff (e.g., 30s, 2m, 10m)
  - `attempts >= max_attempts` → terminal FAILED

---

## 9) LLM Prompts (Strict JSON, Conservative)

### 9.1 Prompt A: Extract (per chunk)
**Rules:**
- Only what is directly supported by text
- Evidence quote <= 25 words + offsets
- JSON only (no markdown)

Output schema:
```json
{
  "entities": [{ "name": "...", "type": "person|org|project|object|place|time", "aliases": ["..."] }],
  "events": [{
    "category": "Decision",
    "subject": { "type": "person|project|object|other", "ref": "..." },
    "actors": [{ "ref": "...", "role": "owner|contributor|reviewer|stakeholder|other" }],
    "event_time": "ISO8601 or null",
    "narrative": "1-2 sentences",
    "evidence": { "quote": "...", "start_char": 0, "end_char": 0 },
    "confidence": 0.0
  }]
}

9.2 Prompt B: Canonicalize (per revision)

Rules:
	•	Do not invent new events
	•	Merge duplicates conservatively
	•	Attach evidence_list per event

Output:

{
  "canonical_events": [{
    "category": "...",
    "subject": {...},
    "actors": [...],
    "event_time": "...",
    "narrative": "...",
    "evidence_list": [{ "chunk_id": "...", "quote": "...", "start_char": 0, "end_char": 0 }],
    "confidence": 0.0
  }]
}


⸻

10) Configuration (Env Vars)

Existing (V2)
	•	OPENAI_API_KEY
	•	OPENAI_EMBED_MODEL=text-embedding-3-large
	•	CHROMA_HOST=chroma
	•	CHROMA_PORT=8000 (internal) / localhost:8100 (host)
	•	SINGLE_PIECE_MAX_TOKENS=1200
	•	CHUNK_TARGET_TOKENS=900
	•	CHUNK_OVERLAP_TOKENS=100

New (V3)
	•	EVENTS_DB_DSN=postgresql://events:events@postgres:5432/events
	•	WORKER_ID=event-worker-1
	•	POLL_INTERVAL_MS=1000
	•	OPENAI_EVENT_MODEL=<your choice> (extraction model, configurable)
	•	EVENT_MAX_ATTEMPTS=5

⸻

11) Implementation Notes That Save Pain

11.1 Treat events as an index, not a replacement
	•	Keep artifacts/chunks as the source of truth for nuance
	•	Events always point back to evidence (revision + chunk + offsets)

11.2 Replace-on-success is simplest
	•	Every successful extraction replaces all events for that revision
	•	Deterministic final state
	•	No duplicate drift

11.3 Keep privacy fields now
	•	Even single-user: store sensitivity/visibility_scope/retention_policy
	•	Retrieval-time enforcement can be added later without schema refactor

⸻

12) End-to-End (E2E) Testing Plan (After V3 Implementation)

12.1 E2E test setup

Start the stack:
	1.	docker compose up -d
	2.	Confirm health:
	•	MCP responds at /mcp/
	•	Chroma reachable
	•	Postgres reachable
	•	event-worker running and polling

12.2 E2E Scenario 1 — Small artifact → immediate chunkless ingest → event job → events available

Steps
	1.	Call artifact_ingest with a short text containing at least 1 clear event:
	•	Example: “Decision: We will use OpenAI text-embedding-3-large starting Jan 1.”
	2.	Verify response contains:
	•	artifact_uid, revision_id, is_chunked=false
	3.	Check job_status for that revision:
	•	Expected: PENDING then DONE
	4.	Call event_search(query="use OpenAI", include_evidence=true)
	•	Expected: 1+ event with:
	•	category = Decision
	•	evidence quote referencing the text
	5.	Call event_get(event_id)
	•	Expected: evidence has correct start_char/end_char and quote ≤ 25 words.

12.3 E2E Scenario 2 — Large artifact → chunking → neighbor evidence still correct

Steps
	1.	Ingest a long doc that forces chunking (>1200 tokens) containing a decision + a commitment.
	2.	Verify is_chunked=true, chunk_count > 1
	3.	Wait for job DONE
	4.	event_search(query="Decision", category="Decision")
	5.	Validate returned evidence includes chunk_id and offsets within that chunk.

12.4 E2E Scenario 3 — Idempotency: re-ingest same content (no duplicate job/events)

Steps
	1.	Re-ingest the exact same artifact (same artifact_uid and same content)
	2.	Expect:
	•	same revision_id
	•	job enqueue is a no-op
	3.	Count events in DB for that revision:
	•	unchanged (no duplicates)

12.5 E2E Scenario 4 — New revision: changed content creates new revision + new events

Steps
	1.	Ingest updated content under the same artifact_uid (same source_id) with a new decision.
	2.	Expect:
	•	new revision_id
	•	new job created
	3.	event_list_for_revision(artifact_uid, revision_id=new)
	•	includes new event(s)
	4.	Confirm previous revision events still exist and remain queryable.

12.6 E2E Scenario 5 — Failure mode: OpenAI failure produces no partial writes

Steps
	1.	Temporarily break OpenAI credentials (or force model name invalid) for worker only.
	2.	Ingest an artifact (job enqueued).
	3.	Worker fails:
	•	job status becomes FAILED (or rescheduled), with error code/message
	4.	Verify Postgres:
	•	no events inserted for that revision
	5.	Restore credentials.
	6.	Force retry (event_reextract or job retry) → DONE
	7.	Verify events now exist.

12.7 E2E Scenario 6 — Performance sanity
	•	Ingest 20 artifacts quickly
	•	Confirm:
	•	MCP remains responsive
	•	worker queue drains
	•	average extraction time acceptable for your usage

⸻

13) “Definition of Done” for V3

V3 is complete when:
	•	Artifacts ingest unchanged from user perspective (still searchable immediately as text)
	•	Every new artifact revision enqueues a durable extraction job
	•	Worker extracts events reliably and stores them in Postgres with evidence pointers
	•	Event tools (event_search, event_get) return usable, auditable results
	•	Idempotency and failure behavior are proven by E2E tests

